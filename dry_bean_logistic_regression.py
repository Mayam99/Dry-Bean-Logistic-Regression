# -*- coding: utf-8 -*-
"""Dry_Bean-Logistic-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHsIMEVvqyRphLUS5LdUo6DiaVvFoCqh

##Introduction
###In this notebook, we will be working on the Dry Bean Dataset from Kaggle, which involves the classification of different types of dry beans. The dataset contains various physical properties of the beans, and our goal is to build a model that can accurately classify the type of bean based on these properties.

##Dataset Overview
###The Dry Bean Dataset consists of 13,611 instances and 17 features, including:

* Area
* Perimeter
* Major Axis Length
* Minor Axis Length
* Aspect Ratio
* Eccentricity
* Convex Area
* Equivalent Diameter
* Extent
* Solidity
* Roundness
* Compactness
* Shape Factor 1
* Shape Factor 2
* Shape Factor 3
* Shape Factor 4
* Class (the type of bean, which is the target variable)

###The dataset contains the following bean types:

* Seker
* Barbunya
* Bombay
* Cali
* Dermason
* Horoz
* Sira

##Objective
###Our primary objective is to develop a logistic regression model to classify the type of bean based on its physical properties. Logistic regression is a popular and straightforward method for classification problems, particularly when the target variable is categorical.

##Steps Involved
###1) Data Exploration and Preprocessing: We'll start by loading the dataset, exploring its structure, and performing necessary preprocessing steps such as handling missing values, normalizing features, and encoding categorical variables.

###2) Model Building: Next, we'll build a logistic regression model using the preprocessed data.

###3) Model Evaluation: We'll evaluate the performance of our model using appropriate metrics such as accuracy, precision, recall, and the F1 score.

###4) Model Optimization: Finally, we'll explore ways to optimize our model, potentially using techniques like hyperparameter tuning and cross-validation.

###By the end of this notebook, we aim to have a robust logistic regression model capable of accurately classifying the different types of dry beans in the dataset.

#Importing Necessary Libraries
###We are required to importing the libraries so as to performing EDA. These include NumPy, Pandas, Matplotlib, and Seaborn.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd # Pandas is a powerful library for data manipulation and analysis.
import numpy as np # NumPy is a powerful tool for numerical computations in Python.
import seaborn as sns # Seaborn is a statistical data visualization library based on Matplotlib
import matplotlib.pyplot as plt # Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python
# %matplotlib inline

"""##Loading the Dataset"""

df= pd.read_csv("Dry_Bean_Dataset.csv") #The df = pd.read_csv line reads a CSV file into a DataFrame named df using the pandas library.

"""##EDA
Exploratory Data Analysis (EDA) is essential for several reasons:

1) Understanding Data Structure: EDA helps you comprehend the data's underlying structure, including the types of features, data types, and relationships between variables.

2) Identifying Patterns and Trends: Visualizations and summary statistics reveal patterns, trends, and anomalies, which can provide insights into the data.

3) Detecting Missing Values and Outliers: EDA allows you to identify missing values, outliers, and errors, which are crucial for data cleaning and ensuring data quality.

4) Feature Selection: By understanding the relationships between features and the target variable, EDA aids in selecting the most relevant features for modeling.

5) Guiding Further Analysis: Insights from EDA can guide the selection of appropriate modeling techniques and preprocessing steps, leading to more effective and accurate models.

In summary, EDA is a critical step in the data science process that ensures a deep understanding of the data, leading to better-informed decisions and more robust models.
"""

df.head() #We will now read the data from a CSV file into a Pandas DataFrame Let us have a look at how our dataset looks like using df.head()

df.columns #Displays the names of the columns

df.shape # Displays the total count of the Rows and Columns respectively.

df.Class.unique()

df['Class'].value_counts() #This code returns the number of occurrences of each unique value in the 'Class' column of the dataframe df.

df.isnull().sum() #Displays the total count of the null values in the particular columns.

df.info() #Displays the total count of values present in the particular column along with the null count and data type.

df[df.duplicated()] #This code returns the count of duplicate rows in the DataFrame df.

df_no_duplicates = df.drop_duplicates() #By dropping duplicate values, we maintain the quality and integrity of your dataset, which is fundamental for accurate data analysis and modeling.

df_no_duplicates.shape #Checking the shape after dropping the values.

df.describe()

sns.pairplot(df, hue='Class')
plt.show()

from sklearn.model_selection import train_test_split #Imports a function to split datasets into training and testing sets.
from sklearn.preprocessing import StandardScaler #Imports a class to standardize features by removing the mean and scaling to unit variance.
from sklearn.preprocessing import LabelEncoder #Imports a class to encode target labels with value between 0 and n_classes 1.

# Encode the labels
label_encoder = LabelEncoder() #Creates an instance of the LabelEncoder class
df['Class'] = label_encoder.fit_transform(df['Class']) #This line encodes the categorical values in the 'Class' column of the DataFrame df into numeric labels. The fit_transform method:

# Separate features and target
X = df.drop('Class', axis=1) #This line creates a new DataFrame X by dropping the 'Class' column from the original DataFrame df
y = df['Class'] #This line assigns the 'Class' column of the original DataFrame df to the variable y. This column represents the target variable or labels that we want to predict.

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#this line of code splits the dataset into training and testing sets, with 80% of the data used for training the model and 20% for testing it, ensuring the split is reproducible.

# Scale the features
scaler = StandardScaler() #This line creates an instance of the StandardScaler class. The StandardScaler standardizes features by removing the mean and scaling to unit variance.
X_train = scaler.fit_transform(X_train)
#This method first fits the scaler to the X_train data by calculating the mean and standard deviation of each feature in the training set. It then transforms the training data by subtracting the mean and scaling to unit variance for each feature.

X_test = scaler.transform(X_test)
#This method transforms the X_test data using the mean and standard deviation calculated from the training set (X_train). It ensures that the test data is scaled in the same way as the training data.

from sklearn.linear_model import LogisticRegression #Imports the LogisticRegression class for creating and training logistic regression models.
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report #Imports functions to evaluate model performance, including accuracy score, confusion matrix, and classification report (precision, recall, F1-score).

# Initialize the model
logistic_model = LogisticRegression(max_iter=1000)

"""####LogisticRegression: Creates an instance of the LogisticRegression class, which is used for performing logistic regression, a type of linear model for binary classification.
####max_iter=1000: Sets the maximum number of iterations for the optimization algorithm to converge. This is used to ensure that the solver has sufficient iterations to find the optimal parameters. If the default maximum iterations (usually 100) are not enough for the model to converge, increasing this number can help.
####In summary, this line of code initializes a logistic regression model with a specified maximum number of iterations set to 1000.
"""

# Train the model
logistic_model.fit(X_train, y_train)

"""####logistic_model: The instance of the LogisticRegression class created earlier.
####fit(X_train, y_train): This method trains the logistic regression model using the training data.
####X_train: The training set features.
####y_train: The training set labels.
####During this process, the model learns the relationship between the features and the target variable by finding the optimal parameters (coefficients) that best fit the training data.

####In summary, this line of code trains the logistic regression model using the provided training data (X_train and y_train).
"""

# Make predictions
y_pred = logistic_model.predict(X_test)

"""####logistic_model: The trained logistic regression model.
####predict(X_test): This method uses the trained model to make predictions on the test set features.
#####X_test: The testing set features.

#####The result is:

#####y_pred: The predicted labels for the test set, based on the learned relationship from the training data.
####In summary, this line of code uses the trained logistic regression model to predict the target labels for the test data and stores these predicted labels in the variable y_pred.
"""

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')
#these lines of code calculate the accuracy of the model's predictions and print it as a percentage.

"""##The logistic regression model achieved an accuracy of 92.66% on the test set."""

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(conf_matrix)

# Classification Report
class_report = classification_report(y_test, y_pred)
print('Classification Report:')
print(class_report)

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear']
    }

# Initialize the GridSearchCV
grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Best parameters and best score
print(f'Best Parameters: {grid_search.best_params_}')
print(f'Best Score: {grid_search.best_score_}')

# Evaluate on test set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
accuracy_best = accuracy_score(y_test, y_pred_best)
print(f'Accuracy with Best Parameters: {accuracy_best * 100:.2f}%')

